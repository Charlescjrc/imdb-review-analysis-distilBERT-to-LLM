{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":12889162,"sourceType":"datasetVersion","datasetId":8154823}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n## 1. ENVIRONMENT SETUP & LIBRARY IMPORTS","metadata":{}},{"cell_type":"code","source":"print(\"▶ Step 1/7: Updating libraries...\")\n!pip install --upgrade -q transformers datasets accelerate scikit-learn\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom datasets import Dataset, DatasetDict\nimport torch\nimport transformers\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback\n)\n\nprint(f\"✔ Transformers version: {transformers.__version__}\")\nif not torch.cuda.is_available(): print(\"WARNING: GPU is not active\")\nelse: print(f\"✔ GPU found: {torch.cuda.get_device_name(0)}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:34:19.019589Z","iopub.execute_input":"2025-08-27T20:34:19.019860Z","iopub.status.idle":"2025-08-27T20:34:36.297347Z","shell.execute_reply.started":"2025-08-27T20:34:19.019835Z","shell.execute_reply":"2025-08-27T20:34:36.296310Z"}},"outputs":[{"name":"stdout","text":"▶ Step 1/7: Updating libraries...\n","output_type":"stream"},{"name":"stderr","text":"2025-08-27 20:34:31.809842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756326871.838327     417 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756326871.846714     417 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✔ Transformers version: 4.56.0.dev0\n✔ GPU found: Tesla T4\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"---\n## 2. DATA LOADING AND CLEANING","metadata":{}},{"cell_type":"code","source":"\nprint(\"▶ Step 2/7: Loading IMDb 50K dataset...\")\n# !!! IMPORTANT: CHANGE THIS FILE PATH TO MATCH YOUR DATASET'S PATH !!!\nfile_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv' \ndf = pd.read_csv(file_path)\n\nprint(\"✔ Dataset loaded successfully.\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:34:36.299581Z","iopub.execute_input":"2025-08-27T20:34:36.300180Z","iopub.status.idle":"2025-08-27T20:34:36.994925Z","shell.execute_reply.started":"2025-08-27T20:34:36.300145Z","shell.execute_reply":"2025-08-27T20:34:36.994008Z"}},"outputs":[{"name":"stdout","text":"▶ Step 2/7: Loading IMDb 50K dataset...\n✔ Dataset loaded successfully.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"---\n## 3. DATA PREPARATION FOR THE MODEL","metadata":{}},{"cell_type":"code","source":"print(\"▶ Step 3/7: Preparing data for model training...\")\n\n# Converting the 'sentiment' column to a numeric 'label' (positive=1, negative=0)\ndf['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n\n# Standardizing the column names\ndf = df[['review', 'label']]\ndf.rename(columns={'review': 'text'}, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\ndataset_dict = DatasetDict({\n    'train': Dataset.from_pandas(train_df),\n    'test': Dataset.from_pandas(test_df)\n})\nprint(\"✔ Training and test sets created.\")\nprint(dataset_dict)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:34:36.996231Z","iopub.execute_input":"2025-08-27T20:34:36.996551Z","iopub.status.idle":"2025-08-27T20:34:37.657078Z","shell.execute_reply.started":"2025-08-27T20:34:36.996525Z","shell.execute_reply":"2025-08-27T20:34:37.656286Z"}},"outputs":[{"name":"stdout","text":"▶ Step 3/7: Preparing data for model training...\n✔ Training and test sets created.\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', '__index_level_0__'],\n        num_rows: 40000\n    })\n    test: Dataset({\n        features: ['text', 'label', '__index_level_0__'],\n        num_rows: 10000\n    })\n})\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"---\n## 4. TOKENIZATION","metadata":{}},{"cell_type":"code","source":"print(\"▶ Step 4/7: Tokenizing the texts...\")\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\ntokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\nprint(\"✔ Tokenization complete.\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:34:37.658006Z","iopub.execute_input":"2025-08-27T20:34:37.658369Z","iopub.status.idle":"2025-08-27T20:35:08.112183Z","shell.execute_reply.started":"2025-08-27T20:34:37.658338Z","shell.execute_reply":"2025-08-27T20:35:08.111312Z"}},"outputs":[{"name":"stdout","text":"▶ Step 4/7: Tokenizing the texts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baad3abac90a4d6d983d86796d2cf3bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec3ccbd1cee4b9092577d774322c53c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03725649736649c78664b7012ec95f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31348511cb8d44fe871485d17be12d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a955fe19ac4c64a0b4301562afa832"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5757fcb20a4bd792f30ee407ef3f8a"}},"metadata":{}},{"name":"stdout","text":"✔ Tokenization complete.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"---\n## 5. PREPARING THE BASE MODEL, METRICS, AND CALLBACK","metadata":{}},{"cell_type":"code","source":"print(\"▶ Step 5/7: Preparing the base model, metrics function, and live monitoring tool...\")\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\nclass CustomProgressCallback(TrainerCallback):\n    \"\"\"\n    A more robust Callback that uses logging events to show progress.\n    \"\"\"\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        \"\"\"\n        This function is called automatically whenever the Trainer creates a log.\n        This is the most reliable place to get the 'loss' value.\n        \"\"\"\n        if state.is_local_process_zero and logs is not None:\n            if 'loss' in logs:\n                print(f\"Step: {state.global_step}/{state.max_steps} | Loss: {logs['loss']:.4f}\")\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        if state.is_local_process_zero:\n            print(f\"\\n--- EPOCH {int(state.epoch)} COMPLETED ---\\n\")\nprint(\"✔ Model, metrics, and monitoring tool are ready.\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:35:08.112916Z","iopub.execute_input":"2025-08-27T20:35:08.113180Z","iopub.status.idle":"2025-08-27T20:35:09.976801Z","shell.execute_reply.started":"2025-08-27T20:35:08.113158Z","shell.execute_reply":"2025-08-27T20:35:09.976086Z"}},"outputs":[{"name":"stdout","text":"▶ Step 5/7: Preparing the base model, metrics function, and live monitoring tool...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307713c6618b4249ac371ba31a83a531"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✔ Model, metrics, and monitoring tool are ready.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"---\n## 6. DEFINING TRAINING ARGUMENTS","metadata":{}},{"cell_type":"code","source":"print(\"▶ Step 6/7: Defining training settings (hyperparameters)...\")\ntraining_args = TrainingArguments(\n    output_dir=\"./imdb_critic_model\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16, # You can try 32 with a T4 x2, but 16 is safer\n    learning_rate=3e-5,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    disable_tqdm=True,\n    report_to=\"none\",\n)\nprint(\"✔ Training settings are complete.\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:35:55.848685Z","iopub.execute_input":"2025-08-27T20:35:55.849008Z","iopub.status.idle":"2025-08-27T20:35:55.889436Z","shell.execute_reply.started":"2025-08-27T20:35:55.848985Z","shell.execute_reply":"2025-08-27T20:35:55.888643Z"}},"outputs":[{"name":"stdout","text":"▶ Step 6/7: Defining training settings (hyperparameters)...\n✔ Training settings are complete.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"---\n## 7. TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"print(\"▶▶▶ Step 7/7: STARTING THE FINE-TUNING PROCESS! ◀◀◀\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[CustomProgressCallback()],\n)\ntrainer.train()\n\nprint(\"\\n🎉 TRAINING COMPLETED SUCCESSFULLY! 🎉\")\nfinal_model_path = \"./imdb_critic_model_best\"\ntrainer.save_model(final_model_path)\nprint(f\"The best model was saved to '{final_model_path}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T20:36:35.485555Z","iopub.execute_input":"2025-08-27T20:36:35.485879Z","iopub.status.idle":"2025-08-27T21:40:11.114629Z","shell.execute_reply.started":"2025-08-27T20:36:35.485858Z","shell.execute_reply":"2025-08-27T21:40:11.113678Z"}},"outputs":[{"name":"stdout","text":"▶▶▶ Step 7/7: STARTING THE FINE-TUNING PROCESS! ◀◀◀\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_417/2413368164.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step: 500/3750 | Loss: 0.2853\n{'loss': 0.2853, 'grad_norm': 444893.6875, 'learning_rate': 2.6008e-05, 'epoch': 0.4}\nStep: 1000/3750 | Loss: 0.2144\n{'loss': 0.2144, 'grad_norm': 379865.21875, 'learning_rate': 2.2008e-05, 'epoch': 0.8}\n\n--- EPOCH 1 COMPLETED ---\n\n{'eval_loss': 0.1719169169664383, 'eval_accuracy': 0.9369, 'eval_runtime': 101.0376, 'eval_samples_per_second': 98.973, 'eval_steps_per_second': 6.186, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step: 1500/3750 | Loss: 0.1595\n{'loss': 0.1595, 'grad_norm': 509357.125, 'learning_rate': 1.8007999999999998e-05, 'epoch': 1.2}\nStep: 2000/3750 | Loss: 0.1279\n{'loss': 0.1279, 'grad_norm': 422271.9375, 'learning_rate': 1.4008e-05, 'epoch': 1.6}\nStep: 2500/3750 | Loss: 0.1302\n{'loss': 0.1302, 'grad_norm': 224262.6875, 'learning_rate': 1.0008e-05, 'epoch': 2.0}\n\n--- EPOCH 2 COMPLETED ---\n\n{'eval_loss': 0.21813704073429108, 'eval_accuracy': 0.9288, 'eval_runtime': 101.1402, 'eval_samples_per_second': 98.873, 'eval_steps_per_second': 6.18, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step: 3000/3750 | Loss: 0.0939\n{'loss': 0.0939, 'grad_norm': 2452.546630859375, 'learning_rate': 6.008000000000001e-06, 'epoch': 2.4}\nStep: 3500/3750 | Loss: 0.1613\n{'loss': 0.1613, 'grad_norm': 207802.09375, 'learning_rate': 2.008e-06, 'epoch': 2.8}\n\n--- EPOCH 3 COMPLETED ---\n\n{'eval_loss': 0.6120579838752747, 'eval_accuracy': 0.9416, 'eval_runtime': 101.0598, 'eval_samples_per_second': 98.951, 'eval_steps_per_second': 6.184, 'epoch': 3.0}\n{'train_runtime': 3813.9987, 'train_samples_per_second': 31.463, 'train_steps_per_second': 0.983, 'train_loss': 0.16715535176595053, 'epoch': 3.0}\n\n🎉 TRAINING COMPLETED SUCCESSFULLY! 🎉\nThe best model was saved to './imdb_critic_model_best'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"---\n## 8. TESTING THE TRAINED MODEL","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nfinal_model_path = \"./imdb_critic_model_best\" \n\n# The device=0 parameter ensures this runs on the GPU.\nimdb_critic_pipeline = pipeline(\"sentiment-analysis\", model=final_model_path, device=0)\n\n# Custom film reviews for testing\ntest_reviews = [\n    \"The plot was predictable and the characters were completely uninteresting.\", # Clearly Negative\n    \"I was on the edge of my seat the entire time! What a masterpiece of cinema.\", # Clearly Positive\n    \"It was an okay movie, not great but not terrible either.\", # Neutral/Ambiguous\n    \"Visually stunning, but the story was surprisingly weak.\" # Mixed Sentiment\n]\n\nprint(\"--- Predictions of the Expert Model on Custom Test Reviews ---\")\nfor review, prediction in zip(test_reviews, imdb_critic_pipeline(test_reviews)):\n    # The model's labels are LABEL_1 (positive) and LABEL_0 (negative)\n    label = \"Positive\" if prediction['label'] == 'LABEL_1' else \"Negative\"\n    score = prediction['score']\n    print(f\"\\nReview: '{review}'\\nPrediction: {label} (Score: {score:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T21:45:43.536704Z","iopub.execute_input":"2025-08-27T21:45:43.537062Z","iopub.status.idle":"2025-08-27T21:45:43.817468Z","shell.execute_reply.started":"2025-08-27T21:45:43.537038Z","shell.execute_reply":"2025-08-27T21:45:43.816606Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"--- Predictions of the Expert Model on Custom Test Reviews ---\n\nReview: 'The plot was predictable and the characters were completely uninteresting.'\nPrediction: Negative (Score: 1.0000)\n\nReview: 'I was on the edge of my seat the entire time! What a masterpiece of cinema.'\nPrediction: Positive (Score: 1.0000)\n\nReview: 'It was an okay movie, not great but not terrible either.'\nPrediction: Negative (Score: 0.9935)\n\nReview: 'Visually stunning, but the story was surprisingly weak.'\nPrediction: Negative (Score: 1.0000)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"---\n## 9. SYNTHESIZING FINDINGS FOR THE LLM","metadata":{}},{"cell_type":"code","source":"# Making Predictions on the Full Dataset with the Expert Model\nprint(\"▶ Analyzing all 50,000 IMDb reviews using the expert model...\")\n\nall_reviews_text = df['text'].tolist()\n\n# FIX: Adding truncation=True to the pipeline call to prevent errors with reviews longer than 512 tokens.\nall_predictions = imdb_critic_pipeline(all_reviews_text, truncation=True)\n\n# Add the predictions and scores to the original DataFrame\ndf['sentiment_label'] = [pred['label'] for pred in all_predictions]\ndf['sentiment_score'] = [pred['score'] for pred in all_predictions]\n\n# Convert LABEL_1 to Positive, LABEL_0 to Negative\ndf['sentiment_label'] = df['sentiment_label'].apply(lambda x: \"Positive\" if x == 'LABEL_1' else \"Negative\")\nprint(\"✔ All reviews have been labeled.\")\nprint(\"-\" * 50)\n\n\n# Summarizing the Findings as Text\nprint(\"▶ Creating summary report for the LLM...\")\n\n# Calculate the overall sentiment distribution\nsentiment_dist = df['sentiment_label'].value_counts(normalize=True) * 100\npositive_percentage = sentiment_dist.get('Positive', 0)\nnegative_percentage = sentiment_dist.get('Negative', 0)\n\n# Find the most confident positive and negative reviews from our model\ntop_positive_review = df[df['sentiment_label'] == 'Positive'].nlargest(1, 'sentiment_score').iloc[0]\ntop_negative_review = df[df['sentiment_label'] == 'Negative'].nlargest(1, 'sentiment_score').iloc[0]\n\n# Combine everything into a single text block\nsummary_for_llm = f\"\"\"\nHere is a data-driven analysis of 50,000 IMDb movie reviews:\n\n**Overall Sentiment Distribution:**\n- Approximately {positive_percentage:.1f}% of the reviews were classified as positive by our expert model.\n- Approximately {negative_percentage:.1f}% of the reviews were classified as negative by our expert model.\n\n**An Excerpt from the Most Confident Positive Review:**\n- Review: \"{top_positive_review['text'][:400]}...\" \n  (Confidence score for being Positive: {top_positive_review['sentiment_score']:.4f})\n\n**An Excerpt from the Most Confident Negative Review:**\n- Review: \"{top_negative_review['text'][:400]}...\"\n  (Confidence score for being Negative: {top_negative_review['sentiment_score']:.4f})\n\"\"\"\n\nprint(\"✔ Summary report is ready!\")\nprint(summary_for_llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T21:45:48.519973Z","iopub.execute_input":"2025-08-27T21:45:48.520582Z","iopub.status.idle":"2025-08-27T21:55:14.753500Z","shell.execute_reply.started":"2025-08-27T21:45:48.520540Z","shell.execute_reply":"2025-08-27T21:55:14.752447Z"}},"outputs":[{"name":"stdout","text":"▶ Analyzing all 50,000 IMDb reviews using the expert model...\n✔ All reviews have been labeled.\n--------------------------------------------------\n▶ Creating summary report for the LLM...\n✔ Summary report is ready!\n\nHere is a data-driven analysis of 50,000 IMDb movie reviews:\n\n**Overall Sentiment Distribution:**\n- Approximately 50.3% of the reviews were classified as positive by our expert model.\n- Approximately 49.7% of the reviews were classified as negative by our expert model.\n\n**An Excerpt from the Most Confident Positive Review:**\n- Review: \"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the referen...\" \n  (Confidence score for being Positive: 1.0000)\n\n**An Excerpt from the Most Confident Negative Review:**\n- Review: \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the e...\"\n  (Confidence score for being Negative: 1.0000)\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"---\n## 10. LOADING THE LARGE LANGUAGE MODEL (LLAMA 3)","metadata":{}},{"cell_type":"code","source":"# Install required libraries from the latest source to ensure compatibility\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q bitsandbytes\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\n# Get the Hugging Face token from Kaggle Secrets\nfrom kaggle_secrets import UserSecretsClient\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\nexcept:\n    print(\"WARNING: 'HUGGING_FACE_HUB_TOKEN' not found in Kaggle Secrets. The model download may fail.\")\n    hf_token = None\n\n# Model ID\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# 4-bit Quantization Settings (to fit the model into GPU memory)\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nprint(f\"▶ Loading the {model_id} model with 4-bit quantization...\")\nprint(\"This process may take a few minutes depending on the model size.\")\n\n# Load the Model and Tokenizer\ntokenizer_llm = AutoTokenizer.from_pretrained(model_id, token=hf_token)\nmodel_llm = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    token=hf_token,\n    quantization_config=quantization_config,\n    device_map=\"auto\", # Automatically distributes the model across available GPUs\n)\n\nprint(\"\\n✔ LLM model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:05:43.521167Z","iopub.execute_input":"2025-08-27T22:05:43.521954Z","iopub.status.idle":"2025-08-27T22:07:38.361316Z","shell.execute_reply.started":"2025-08-27T22:05:43.521926Z","shell.execute_reply":"2025-08-27T22:07:38.360468Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n▶ Loading the meta-llama/Meta-Llama-3-8B-Instruct model with 4-bit quantization...\nThis process may take a few minutes depending on the model size.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"659818da62894fec8f7291621cfb0ebb"}},"metadata":{}},{"name":"stdout","text":"\n✔ LLM model loaded successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"---\n## 11. GENERATING THE FINAL CRITIQUE","metadata":{}},{"cell_type":"code","source":"print(\"▶ Preparing the prompt for the Master Writer (Llama 3)...\")\n\n# Prepare the special prompt format for Llama 3\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a world-class movie critic known for your deep and insightful analysis. Your task is to write a comprehensive, multi-paragraph critique based ONLY on the data-driven summary of 50,000 IMDb movie reviews provided by your research analyst.\",\n    },\n    {\n        \"role\": \"user\", \n        \"content\": summary_for_llm # <-- The summary we created earlier!\n    },\n]\n\n# Convert the prompt into the format the model understands\nprompt = tokenizer_llm.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer_llm(prompt, return_tensors=\"pt\").to(\"cuda\")\n\nprint(\"\\n▶▶▶ The Master Writer (Llama 3) is generating the critique...\")\nprint(\"This process may take a few minutes.\")\n\n# Generate text by running the model\noutputs = model_llm.generate(\n    **inputs,\n    max_new_tokens=1024, # Maximum number of new words/tokens to generate\n    do_sample=True,      # Use sampling to increase creativity\n    temperature=0.7,     # Lower temperature for more focused and less random text (0.1 - 1.0)\n    top_p=0.9,           # Select from the most probable words\n)\n\n# Decode the generated text (clean it up)\nresponse_text = tokenizer_llm.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\n--- FINAL REPORT FROM THE AI CRITIC ---\")\nprint(response_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:09:09.808539Z","iopub.execute_input":"2025-08-27T22:09:09.809345Z","iopub.status.idle":"2025-08-27T22:10:00.144292Z","shell.execute_reply.started":"2025-08-27T22:09:09.809315Z","shell.execute_reply":"2025-08-27T22:10:00.143482Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"▶ Preparing the prompt for the Master Writer (Llama 3)...\n\n▶▶▶ The Master Writer (Llama 3) is generating the critique...\nThis process may take a few minutes.\n\n--- FINAL REPORT FROM THE AI CRITIC ---\nTitle: A Tale of Two Cinemas: A Critique of the Duality of Movie Reviews\n\nIn the vast expanse of the cinematic universe, it's rare to find a film that elicits a universal response from its audience. The 50,000 IMDb reviews analyzed by our research analyst reveal a fascinating dichotomy: half of the reviews are positive, while the other half are negative. This stark contrast begs the question: what drives this divide? Is it a reflection of the filmmakers' artistic vision, the audience's subjective experience, or something more nuanced?\n\nUpon closer examination, the data suggests that the most confident positive reviews, exemplified by the excerpt provided, praise the film's technical aspects, such as cinematography and editing. The use of \"old-time-BBC fashion\" is described as \"comforting\" and \"realistic,\" implying that the filmmakers have successfully transported the audience to a bygone era. The actors, particularly Michael Sheen, are lauded for their convincing performances, which \"give a sense of realism\" to the film. These reviews demonstrate a deep appreciation for the craft, with critics praising the attention to detail and commitment to authenticity.\n\nOn the other hand, the most confident negative reviews, exemplified by the excerpt provided, are scathing in their criticism. The reviewer describes the film as \"awful\" in almost every aspect, from editing and pacing to storyline and acting. The use of the phrase \"bad mistake\" implies a sense of regret and disappointment, as if the reviewer had been duped by the positive reviews. The film is characterized as \"cheap\" and \"nasty,\" with a soundtrack that is deemed \"lame\" and overplayed. These reviews demonstrate a visceral reaction to the film's perceived shortcomings, with critics lambasting the production for its technical and artistic failings.\n\nThe dichotomy between these two extremes raises questions about the nature of criticism and the role of audience expectations. Do positive reviews reflect a genuine appreciation for the film's artistic merits, or are they influenced by factors such as marketing hype or peer pressure? Conversely, do negative reviews reflect a genuine disappointment with the film's execution, or are they motivated by a desire to counterbalance the positive sentiment?\n\nUltimately, the analysis of these 50,000 reviews serves as a reminder that film criticism is a highly subjective and personal experience. While it's impossible to reconcile the divergent opinions, it's clear that the cinematic landscape is a complex tapestry of perspectives and opinions. As critics, it's our duty to engage with these diverse viewpoints, acknowledging the value of both the positive and negative reviews in our quest to understand the art of filmmaking.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"---\n## 12. THE \"CRITIC AGENT\" FUNCTION","metadata":{}},{"cell_type":"code","source":"def ask_critic_agent(question, summary_text):\n    \"\"\"\n    Sends the user's question and the summary text to Llama 3\n    and gets a response from the \"critic agent\" persona.\n    \"\"\"\n    \n    # Ensure the summary text is not empty\n    if not summary_text:\n        print(\"ERROR: 'summary_text' is empty. Please ensure the summary was generated correctly.\")\n        return\n\n    # Prepare the prompt format for Llama 3\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world-class movie critic. Your task is to answer questions and provide analysis based ONLY on the data-driven summary of 50,000 IMDb movie reviews that will be provided. Do not use any external knowledge.\",\n        },\n        {\n            \"role\": \"user\", \n            # We combine the summary data with the new question here\n            \"content\": f\"Here is the data summary:\\n{summary_text}\\n\\nNow, based ONLY on that summary, answer the following question: {question}\"\n        },\n    ]\n\n    prompt = tokenizer_llm.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer_llm(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    print(f\"▶ The Critic Agent is thinking about the question: '{question[:50]}...'\")\n    outputs = model_llm.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n    )\n    \n    # Decode the generated text\n    response_text = tokenizer_llm.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    \n    print(\"\\n--- The Critic Agent's Answer ---\")\n    print(response_text)\n\n# --- EXAMPLE OF HOW TO USE THE FUNCTION ---\n# This assumes 'summary_for_llm' was created in a previous cell.\n\n# my_question = \"What is the main theme of the most","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:10:24.233563Z","iopub.execute_input":"2025-08-27T22:10:24.233888Z","iopub.status.idle":"2025-08-27T22:10:24.240364Z","shell.execute_reply.started":"2025-08-27T22:10:24.233865Z","shell.execute_reply":"2025-08-27T22:10:24.239418Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"---\n## 13. INTERACTING WITH THE CRITIC AGENT","metadata":{}},{"cell_type":"code","source":"question = \"Based on the overall 50/50 sentiment, would you describe the audience's reaction as 'controversial'?\"\nask_critic_agent(question,summary_for_llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:10:29.046760Z","iopub.execute_input":"2025-08-27T22:10:29.047150Z","iopub.status.idle":"2025-08-27T22:10:40.795673Z","shell.execute_reply.started":"2025-08-27T22:10:29.047102Z","shell.execute_reply":"2025-08-27T22:10:40.794721Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"▶ The Critic Agent is thinking about the question: 'Based on the overall 50/50 sentiment, would you de...'\n\n--- The Critic Agent's Answer ---\nBased solely on the data summary, I would not describe the audience's reaction as \"controversial\". The data shows that the sentiment is evenly split, with 50.3% of reviews being positive and 49.7% being negative. This suggests that the audience is generally divided in their opinions, with no clear majority leaning towards either side. In the absence of any significant skew towards one extreme, I would not characterize the reaction as \"controversial\", which typically implies a strong and divisive opinion.\n","output_type":"stream"}],"execution_count":14}]}